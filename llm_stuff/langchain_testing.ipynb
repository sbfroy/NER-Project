{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from src.utils.config_loader import load_config\n",
    "\n",
    "base_dir = Path(os.getcwd()).parent\n",
    "\n",
    "config = load_config(base_dir / 'secrets.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.preprocessing import create_df\n",
    "\n",
    "val_df = create_df(base_dir / 'data/my_data/regplans-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import (SystemMessage, HumanMessage, AIMessage)\n",
    "\n",
    "os.environ['OPENAI_API_VERSION'] = config['OPENAI_API_VERSION']\n",
    "os.environ['AZURE_OPENAI_ENDPOINT'] = config['OPENAI_API_BASE']\n",
    "os.environ['AZURE_OPENAI_API_KEY'] = config['OPENAI_API_KEY']\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=config['OPENAI_DEPLOYMENT_NAME']\n",
    ")#.bind(response_format={\"type\": \"json_object\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(base_dir / \\'llm_stuff/prompts/examples.json\\', \\'r\\') as f:\\n    example_bank = json.load(f)\\n\\ndef format_examples(example_subset): \\n    # Formats the examples into a string for later prompt\\n    formatted = []\\n    for i, ex in enumerate(example_subset):\\n        entity_lines = \"\\n\".join([f\"{e[\\'word\\']} {e[\\'label\\']}\" for e in ex[\"entities\"]])\\n        formatted.append(f\"Example {i+1}:\\nSentence: \"{ex[\\'sentence\\']}\"\\nEntities:\\n{entity_lines}\\n\")\\n    \\n    return \"\\n\".join(formatted)\\n\\nthe_examples = random.sample(example_bank, 5)\\nformatted_examples = format_examples(the_examples)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\"\"\"\n",
    "with open(base_dir / 'llm_stuff/prompts/examples.json', 'r') as f:\n",
    "    example_bank = json.load(f)\n",
    "\n",
    "def format_examples(example_subset): \n",
    "    # Formats the examples into a string for later prompt\n",
    "    formatted = []\n",
    "    for i, ex in enumerate(example_subset):\n",
    "        entity_lines = \"\\n\".join([f\"{e['word']} {e['label']}\" for e in ex[\"entities\"]])\n",
    "        formatted.append(f\"Example {i+1}:\\nSentence: \\\"{ex['sentence']}\\\"\\nEntities:\\n{entity_lines}\\n\")\n",
    "    \n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "the_examples = random.sample(example_bank, 5)\n",
    "formatted_examples = format_examples(the_examples)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from src.utils.label_mapping_regplans import label_to_id\\nfrom collections import defaultdict\\nfrom textwrap import dedent\\n\\nall_pred_ids = []\\nall_true_ids = []\\nall_results = []\\n\\nfor idx, row in tqdm(val_df.iterrows(), total=len(val_df)):\\n\\n    sentence = row[\\'full_text\\']\\n    tokens = row[\\'words\\']\\n    true_labels = row[\\'labels\\']  \\n\\n    msg = [\\n    SystemMessage(\\n        f\"\"\"You are an expert in Natural Language Processing. Your task is to identify Named Entities (NER) in a given text.\\n            The possible Named Entities are exclusively \\'B-FELT\\' and \\'I-FELT\\'. The entities are defined as follows:\\n\\n            - B-FELT: The beginning of a field zone name.\\n            - I-FELT: The continuation of a field zone name.    \\n\\n            Important Rules:\\n            - A B-FELT must always appear before an I-FELT.\\n            - An I-FELT cannot exist without a preceding B-FELT.\\n                                \\n            Below are some examples of sentences with their corresponding entities:\\n\\n            {formatted_examples}\\n        \"\"\"\\n    ),\\n    HumanMessage(f\"Your task is to identify the Named Entities in the following sentence: \\'{sentence}\\'\") ]\\n\\n    response = llm.invoke(msg)\\n\\n    entities = defaultdict(list) # Word-label pairs\\n\\n    for line in response.content.splitlines():\\n        parts = line.strip().split()\\n        if len(parts) == 2:\\n            word, label = parts[0], parts[1]\\n            entities[word].append(label)\\n\\n    pred_labels = []\\n    word_counts = defaultdict(int)  # Track occurrences of each word\\n\\n    for token in tokens:\\n        if token in entities and word_counts[token] < len(entities[token]):\\n            pred_labels.append(entities[token][word_counts[token]])  # Get the label in order\\n            word_counts[token] += 1  # Increment occurrence counter\\n        else:\\n            pred_labels.append(\"O\")  # Default to \"O\" if missing\\n\\n    # Convert labels to IDs\\n    pred_ids = []\\n    for label in pred_labels:\\n        if label in label_to_id:\\n            pred_ids.append(label_to_id[label])\\n        else:\\n            print(f\"Warning: Unexpected label \\'{label}\\' found. Assigning default label \\'O\\'.\")\\n            pred_ids.append(label_to_id.get(\"O\", -1))\\n\\n    true_ids = [label_to_id[label] for label in true_labels]\\n\\n    all_pred_ids.extend(pred_ids)\\n    all_true_ids.extend(true_ids)\\n\\n    all_results.append({\\n        \\'sentence\\': sentence,\\n        \\'tokens\\': tokens,\\n        \\'true_labels\\': true_labels,\\n        \\'predicted_labels\\': pred_labels,\\n        \\'generated_text\\': response.content\\n    })'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from src.utils.label_mapping_regplans import label_to_id\n",
    "from collections import defaultdict\n",
    "from textwrap import dedent\n",
    "\n",
    "all_pred_ids = []\n",
    "all_true_ids = []\n",
    "all_results = []\n",
    "\n",
    "for idx, row in tqdm(val_df.iterrows(), total=len(val_df)):\n",
    "\n",
    "    sentence = row['full_text']\n",
    "    tokens = row['words']\n",
    "    true_labels = row['labels']  \n",
    "\n",
    "    msg = [\n",
    "    SystemMessage(\n",
    "        f\"\"\"You are an expert in Natural Language Processing. Your task is to identify Named Entities (NER) in a given text.\n",
    "            The possible Named Entities are exclusively 'B-FELT' and 'I-FELT'. The entities are defined as follows:\n",
    "\n",
    "            - B-FELT: The beginning of a field zone name.\n",
    "            - I-FELT: The continuation of a field zone name.    \n",
    "\n",
    "            Important Rules:\n",
    "            - A B-FELT must always appear before an I-FELT.\n",
    "            - An I-FELT cannot exist without a preceding B-FELT.\n",
    "                                \n",
    "            Below are some examples of sentences with their corresponding entities:\n",
    "\n",
    "            {formatted_examples}\n",
    "        \"\"\"\n",
    "    ),\n",
    "    HumanMessage(f\"Your task is to identify the Named Entities in the following sentence: '{sentence}'\") ]\n",
    "\n",
    "    response = llm.invoke(msg)\n",
    "\n",
    "    entities = defaultdict(list) # Word-label pairs\n",
    "\n",
    "    for line in response.content.splitlines():\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            word, label = parts[0], parts[1]\n",
    "            entities[word].append(label)\n",
    "\n",
    "    pred_labels = []\n",
    "    word_counts = defaultdict(int)  # Track occurrences of each word\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in entities and word_counts[token] < len(entities[token]):\n",
    "            pred_labels.append(entities[token][word_counts[token]])  # Get the label in order\n",
    "            word_counts[token] += 1  # Increment occurrence counter\n",
    "        else:\n",
    "            pred_labels.append(\"O\")  # Default to \"O\" if missing\n",
    "\n",
    "    # Convert labels to IDs\n",
    "    pred_ids = []\n",
    "    for label in pred_labels:\n",
    "        if label in label_to_id:\n",
    "            pred_ids.append(label_to_id[label])\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected label '{label}' found. Assigning default label 'O'.\")\n",
    "            pred_ids.append(label_to_id.get(\"O\", -1))\n",
    "\n",
    "    true_ids = [label_to_id[label] for label in true_labels]\n",
    "\n",
    "    all_pred_ids.extend(pred_ids)\n",
    "    all_true_ids.extend(true_ids)\n",
    "\n",
    "    all_results.append({\n",
    "        'sentence': sentence,\n",
    "        'tokens': tokens,\n",
    "        'true_labels': true_labels,\n",
    "        'predicted_labels': pred_labels,\n",
    "        'generated_text': response.content\n",
    "    })'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/355 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 81/355 [09:04<30:43,  6.73s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sbfro\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\openai\\_base_client.py:1056\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1056\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sbfro\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\httpx\\_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[1;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://kartai-openai.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 61\u001b[0m\n\u001b[0;32m     13\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]  \n\u001b[0;32m     15\u001b[0m msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m SystemMessage(\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an expert in Natural Language Processing. Your task is to identify Named Entities (NER) in a given text.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m ),\n\u001b[0;32m     59\u001b[0m HumanMessage(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour task is to identify the Named Entities in the following sentence: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) ]\n\u001b[1;32m---> 61\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(msg)\n\u001b[0;32m     63\u001b[0m entities \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m) \u001b[38;5;66;03m# Word-label pairs\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39msplitlines():\n",
      "File \u001b[1;32mc:\\Users\\sbfro\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    283\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 284\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    285\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    286\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    287\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    288\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    289\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    290\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    291\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    292\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    293\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\sbfro\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:860\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    854\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    859\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sbfro\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:690\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 690\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    691\u001b[0m                 m,\n\u001b[0;32m    692\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    693\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    694\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    695\u001b[0m             )\n\u001b[0;32m    696\u001b[0m         )\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\sbfro\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:925\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 925\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    926\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    927\u001b[0m         )\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    929\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sbfro\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:800\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    798\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 800\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[1;32mc:\\Users\\sbfro\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sbfro\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:879\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    876\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    877\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    878\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    880\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    881\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    882\u001b[0m             {\n\u001b[0;32m    883\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    884\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    885\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m    886\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    887\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    888\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    889\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    890\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    891\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    892\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    893\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    894\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m    895\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    896\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    897\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m    898\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    899\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    900\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    901\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    902\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    903\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    904\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    906\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    907\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    908\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    909\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    910\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    911\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    912\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    913\u001b[0m             },\n\u001b[0;32m    914\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    915\u001b[0m         ),\n\u001b[0;32m    916\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    917\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    918\u001b[0m         ),\n\u001b[0;32m    919\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    920\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    921\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    922\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sbfro\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\openai\\_base_client.py:1296\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1284\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1292\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1293\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1294\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1295\u001b[0m     )\n\u001b[1;32m-> 1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\sbfro\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\openai\\_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    971\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    974\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    975\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    976\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    977\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    978\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    979\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sbfro\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\openai\\_base_client.py:1062\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1061\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1063\u001b[0m         input_options,\n\u001b[0;32m   1064\u001b[0m         cast_to,\n\u001b[0;32m   1065\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1066\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1067\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1068\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1069\u001b[0m     )\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\sbfro\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\openai\\_base_client.py:1109\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1105\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m-> 1109\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1112\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1113\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1116\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1117\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.utils.label_mapping_regplans import label_to_id\n",
    "from collections import defaultdict\n",
    "\n",
    "all_pred_ids = []\n",
    "all_true_ids = []\n",
    "all_results = []\n",
    "\n",
    "# Only do the anlaysis on 25 % of the data\n",
    "val_df = val_df.iloc[:int(len(val_df) * 0.25)]\n",
    "\n",
    "for idx, row in tqdm(val_df.iterrows(), total=len(val_df)):\n",
    "\n",
    "    sentence = row['full_text']\n",
    "    tokens = row['words']\n",
    "    true_labels = row['labels']  \n",
    "\n",
    "    msg = [\n",
    "    SystemMessage(\n",
    "        f\"\"\"You are an expert in Natural Language Processing. Your task is to identify Named Entities (NER) in a given text.\n",
    "            The possible Named Entities are exclusively 'B-FELT' and 'I-FELT'. The entities are defined as follows:\n",
    "\n",
    "            - B-FELT: The beginning of a field zone name.\n",
    "            - I-FELT: The continuation of a field zone name.    \n",
    "\n",
    "            Important Rules:\n",
    "            - A B-FELT must always appear before an I-FELT.\n",
    "            - An I-FELT cannot exist without a preceding B-FELT.\n",
    "                                \n",
    "            Below are some examples of sentences with their corresponding entities:\n",
    "            \n",
    "            Example 1:\n",
    "            Sentence: \"Adkomst til BFS1 og BFS2 skal være fra Solfjellveien .\"\n",
    "            Entities:\n",
    "            BFS1 B-FELT\n",
    "            BFS2 B-FELT\n",
    "                \n",
    "            Example 2:\n",
    "            Sentence: \"På friområdene GF1 - GF3 tillates vanlig skjøtsel av trær og vegetasjon .\"\n",
    "            Entities:\n",
    "            GF1 B-FELT\n",
    "            - I-FELT\n",
    "            GF3 I-FELT\n",
    "\n",
    "            Example 3: \n",
    "            Sentence: \"Bebyggelsestype Innenfor BKS1-BKS6 og BFS2 skal det oppføres flermannsboliger , kjedeboliger og / eller rekkehus .\"\n",
    "            Entities:\n",
    "            BKS1-BKS6 B-FELT\n",
    "            BFS2 B-FELT\n",
    "\n",
    "            Example 4:\n",
    "            Sentence: \"Sonene med nemningane # 1 , # 2 og # 3 gjeld automatisk freda kulturminne , dyrkingsspor med id .\"\n",
    "            Entities:\n",
    "            # B-FELT\n",
    "            1 I-FELT\n",
    "            # B-FELT\n",
    "            2 I-FELT\n",
    "            # B-FELT\n",
    "            3 I-FELT\n",
    "        \"\"\"\n",
    "    ),\n",
    "    HumanMessage(f\"Your task is to identify the Named Entities in the following sentence: '{sentence}'\") ]\n",
    "\n",
    "    response = llm.invoke(msg)\n",
    "\n",
    "    entities = defaultdict(list) # Word-label pairs\n",
    "\n",
    "    for line in response.content.splitlines():\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            word, label = parts[0], parts[1]\n",
    "            entities[word].append(label)\n",
    "\n",
    "    pred_labels = []\n",
    "    word_counts = defaultdict(int)  # Track occurrences of each word\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in entities and word_counts[token] < len(entities[token]):\n",
    "            pred_labels.append(entities[token][word_counts[token]])  # Get the label in order\n",
    "            word_counts[token] += 1  # Increment occurrence counter\n",
    "        else:\n",
    "            pred_labels.append(\"O\")  # Default to \"O\" if missing\n",
    "\n",
    "    # Convert labels to IDs\n",
    "    pred_ids = []\n",
    "    for label in pred_labels:\n",
    "        if label in label_to_id:\n",
    "            pred_ids.append(label_to_id[label])\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected label '{label}' found. Assigning default label 'O'.\")\n",
    "            pred_ids.append(label_to_id.get(\"O\", -1))\n",
    "\n",
    "    true_ids = [label_to_id[label] for label in true_labels]\n",
    "\n",
    "    all_pred_ids.extend(pred_ids)\n",
    "    all_true_ids.extend(true_ids)\n",
    "\n",
    "    all_results.append({\n",
    "        'sentence': sentence,\n",
    "        'tokens': tokens,\n",
    "        'true_labels': true_labels,\n",
    "        'predicted_labels': pred_labels,\n",
    "        'generated_text': response.content\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics on Dev Set:\n",
      "{'precision': 0.8597659468650818, 'recall': 0.7329059839248657, 'f1': 0.7816230654716492, 'span_acc': 0.7769230604171753, 'classification_report': {'B-FELT': {'precision': 0.9298245614035088, 'recall': 0.8153846153846154, 'f1-score': 0.8688524590163934, 'support': 130.0}, 'I-FELT': {'precision': 0.6875, 'recall': 0.39285714285714285, 'f1-score': 0.5, 'support': 28.0}, 'O': {'precision': 0.9619732785200411, 'recall': 0.9904761904761905, 'f1-score': 0.9760166840458812, 'support': 945.0}, 'accuracy': 0.9546690843155031, 'macro avg': {'precision': 0.8597659466411832, 'recall': 0.732905982905983, 'f1-score': 0.7816230476874249, 'support': 1103.0}, 'weighted avg': {'precision': 0.9512166284532141, 'recall': 0.9546690843155031, 'f1-score': 0.9513024352633623, 'support': 1103.0}}}\n"
     ]
    }
   ],
   "source": [
    "from llm_stuff.evaluation import evaluate \n",
    "\n",
    "metrics = evaluate(all_true_ids, all_pred_ids)\n",
    "\n",
    "print(\"Evaluation Metrics on Dev Set:\")\n",
    "print(metrics)\n",
    "\n",
    "final_output = {\n",
    "    'prompt': str(msg),\n",
    "    'evaluation_metrics': metrics,\n",
    "    'results': all_results\n",
    "}\n",
    "\n",
    "with open(base_dir / f'llm_stuff/results/{config['OPENAI_DEPLOYMENT_NAME']}_PROMPT_V1.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_output, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
