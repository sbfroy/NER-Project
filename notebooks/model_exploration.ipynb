{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(os.getcwd()).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from src.data.dataset import Dataset\n",
    "from src.data.preprocessing import create_df\n",
    "from src.utils.label_mapping import label_to_id, id_to_label\n",
    "from src.utils.config_loader import load_config\n",
    "from src.utils.seed import seed_everything\n",
    "from src.models.transformer_model import TransformerModel\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(base_dir / 'model_params.yaml')\n",
    "\n",
    "seed_everything(config['general']['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['model']['model_name'])\n",
    "\n",
    "# Data loading\n",
    "df = create_df(base_dir / 'data/my_data/all_regplans.conllu')\n",
    "dataset = Dataset(df, tokenizer, config['data']['max_seq_len'])\n",
    "loader = DataLoader(dataset, batch_size=config['training']['general']['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sbfro\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\sbfro\\AppData\\Local\\Temp\\ipykernel_31180\\3262077093.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(base_dir / 'src/models/transformer_model.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (transformer): BertForTokenClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.45, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=19, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerModel(\n",
    "    model_name=config['model']['model_name'], \n",
    "    dropout=config['model']['dropout'],\n",
    "    num_labels=len(label_to_id)\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.load_state_dict(torch.load(base_dir / 'src/models/transformer_model.pth', map_location=device)) \n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [01:31<00:00,  4.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "b_felt_words = []\n",
    "b_felt_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(df))):\n",
    "        sentence = df.iloc[i]\n",
    "        words = sentence['words']\n",
    "        labels = sentence['labels']\n",
    "\n",
    "        encoding = tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            max_length=config['data']['max_seq_len']\n",
    "        )\n",
    "\n",
    "        inputs = encoding['input_ids'].to(device)\n",
    "        masks = encoding['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(inputs, masks, labels=None)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).squeeze(0).tolist()\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs.squeeze(0))\n",
    "        pred_labels = [id_to_label.get(pred, 'O') for pred in preds] # Back to labels, O if not in label_to_id\n",
    "\n",
    "        aligned_preds = []\n",
    "        aligned_words = []\n",
    "        word_idx = -1\n",
    "\n",
    "        offsets = encoding['offset_mapping'].squeeze(0)\n",
    "\n",
    "        # Align predictions with words\n",
    "        for j, (token, offset) in enumerate(zip(tokens, offsets)):\n",
    "            if offset[0] == 0 and offset[1] != 0: \n",
    "                word_idx += 1\n",
    "                if word_idx < len(words):\n",
    "                    aligned_words.append(words[word_idx])\n",
    "                    aligned_preds.append(pred_labels[j])\n",
    "        \n",
    "        # Keep only B-FELT labeled words and their predictions\n",
    "        for word, label, pred in zip(aligned_words, labels, aligned_preds):\n",
    "            if label == 'B-FELT':\n",
    "                b_felt_words.append(word)\n",
    "                b_felt_preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions for B-FELT words:\n",
      "Word: B1 -> Predicted Entity: O\n",
      "Word: B4 -> Predicted Entity: O\n",
      "Word: BKS1-6 -> Predicted Entity: B-LOC\n",
      "Word: BB1 -> Predicted Entity: B-LOC\n",
      "Word: BFS1-3 -> Predicted Entity: B-LOC\n",
      "Word: BKS1-6 -> Predicted Entity: B-PROD\n",
      "Word: BKS -> Predicted Entity: B-ORG\n",
      "Word: BFS1 -> Predicted Entity: B-LOC\n",
      "Word: BFS6 -> Predicted Entity: B-LOC\n",
      "Word: BR -> Predicted Entity: O\n",
      "Word: BIA -> Predicted Entity: B-ORG\n",
      "Word: GF1 -> Predicted Entity: B-LOC\n",
      "Word: GF3 -> Predicted Entity: B-LOC\n",
      "Word: BE -> Predicted Entity: O\n",
      "Word: VFS1 -> Predicted Entity: B-LOC\n",
      "Word: VFS2 -> Predicted Entity: B-LOC\n",
      "Word: H140_1 -> Predicted Entity: O\n",
      "Word: H140_4 -> Predicted Entity: O\n",
      "Word: o_GF1-3 -> Predicted Entity: O\n",
      "Word: H320 -> Predicted Entity: O\n",
      "Word: o_GTD1-2 -> Predicted Entity: O\n",
      "Word: B1 -> Predicted Entity: B-PROD\n",
      "Word: B4 -> Predicted Entity: O\n",
      "Word: BKS1 -> Predicted Entity: B-PROD\n",
      "Word: BFS1 -> Predicted Entity: B-PROD\n",
      "Word: BFS6 -> Predicted Entity: B-PROD\n",
      "Word: BKS -> Predicted Entity: B-ORG\n",
      "Word: o_SV3 -> Predicted Entity: O\n",
      "Word: BFS2 -> Predicted Entity: B-LOC\n",
      "Word: BKS4 -> Predicted Entity: B-PROD\n",
      "Word: BKS6 -> Predicted Entity: B-LOC\n",
      "Word: BKS -> Predicted Entity: B-LOC\n",
      "Word: GF1 -> Predicted Entity: B-LOC\n",
      "Word: BKS -> Predicted Entity: B-LOC\n",
      "Word: o_GTD1-2 -> Predicted Entity: O\n",
      "Word: BKS -> Predicted Entity: B-ORG\n",
      "Word: o_SF -> Predicted Entity: B-ORG\n",
      "Word: BR -> Predicted Entity: B-LOC\n",
      "Word: o_SF -> Predicted Entity: O\n",
      "Word: o_SPA -> Predicted Entity: O\n",
      "Word: fylkesveg -> Predicted Entity: O\n",
      "Word: BKS1 -> Predicted Entity: B-LOC\n",
      "Word: BKS6 -> Predicted Entity: B-LOC\n",
      "Word: BFS2 -> Predicted Entity: B-LOC\n",
      "Word: BKS2 -> Predicted Entity: B-PROD\n",
      "Word: BB1 -> Predicted Entity: B-LOC\n",
      "Word: BSM1 -> Predicted Entity: B-LOC\n",
      "Word: BSM2 -> Predicted Entity: B-LOC\n",
      "Word: BFS1 -> Predicted Entity: B-LOC\n",
      "Word: BFS3 -> Predicted Entity: B-LOC\n",
      "Word: BKS1 -> Predicted Entity: B-LOC\n",
      "Word: B1 -> Predicted Entity: O\n",
      "Word: B4 -> Predicted Entity: O\n",
      "Word: BKS2 -> Predicted Entity: B-LOC\n",
      "Word: B1 -> Predicted Entity: B-LOC\n",
      "Word: B4 -> Predicted Entity: O\n",
      "Word: BKS3 -> Predicted Entity: B-LOC\n",
      "Word: BKS4 -> Predicted Entity: B-LOC\n",
      "Word: BKS5 -> Predicted Entity: B-LOC\n",
      "Word: BKS6 -> Predicted Entity: B-LOC\n",
      "Word: o_SKV -> Predicted Entity: O\n",
      "Word: BB1 -> Predicted Entity: B-LOC\n",
      "Word: o_SKV -> Predicted Entity: B-LOC\n",
      "Word: BFS1 -> Predicted Entity: B-LOC\n",
      "Word: BFS3 -> Predicted Entity: B-LOC\n",
      "Word: #01 -> Predicted Entity: O\n",
      "Word: o_SGS -> Predicted Entity: O\n",
      "Word: o_SGS -> Predicted Entity: O\n",
      "Word: BFS2 -> Predicted Entity: B-LOC\n",
      "Word: o_SKV1 -> Predicted Entity: O\n",
      "Word: BB1 -> Predicted Entity: B-LOC\n",
      "Word: BFS1 -> Predicted Entity: B-LOC\n",
      "Word: BFS6 -> Predicted Entity: B-LOC\n",
      "Word: BKS -> Predicted Entity: B-LOC\n",
      "Word: SKV2 -> Predicted Entity: B-LOC\n",
      "Word: BFS1 -> Predicted Entity: B-PROD\n",
      "Word: H370 -> Predicted Entity: B-LOC\n",
      "Word: BKS1-6 -> Predicted Entity: B-LOC\n",
      "Word: BFS1-3 -> Predicted Entity: B-LOC\n",
      "Word: VAA -> Predicted Entity: B-LOC\n",
      "Word: BB1 -> Predicted Entity: O\n",
      "Word: H140 -> Predicted Entity: O\n",
      "Word: H320 -> Predicted Entity: O\n",
      "Word: BKS1 -> Predicted Entity: B-PROD\n",
      "Word: BKS1 -> Predicted Entity: B-LOC\n",
      "Word: BKS2 -> Predicted Entity: B-LOC\n",
      "Word: fv.42 -> Predicted Entity: O\n",
      "Word: fv. -> Predicted Entity: O\n",
      "Word: fylkesveg -> Predicted Entity: O\n",
      "Word: BKS1 -> Predicted Entity: B-ORG\n",
      "Word: o_SGS -> Predicted Entity: O\n",
      "Word: fv. -> Predicted Entity: O\n",
      "Word: BKS -> Predicted Entity: B-LOC\n",
      "Word: BKS1-6 -> Predicted Entity: B-PROD\n",
      "Word: BB1 -> Predicted Entity: B-LOC\n",
      "Word: BKS -> Predicted Entity: B-LOC\n",
      "Word: BFS1 -> Predicted Entity: B-PROD\n",
      "Word: BFS3 -> Predicted Entity: B-PROD\n",
      "Word: BG1 -> Predicted Entity: O\n",
      "Word: BG2 -> Predicted Entity: O\n",
      "Word: BB1 -> Predicted Entity: B-LOC\n",
      "Word: BLK1 -> Predicted Entity: B-LOC\n",
      "Word: BLK2 -> Predicted Entity: B-LOC\n",
      "Word: BFS1 -> Predicted Entity: B-LOC\n",
      "Word: BFS3 -> Predicted Entity: B-LOC\n",
      "Word: BB1 -> Predicted Entity: B-LOC\n",
      "Word: BFS1 -> Predicted Entity: B-LOC\n",
      "Word: BKS1 -> Predicted Entity: B-LOC\n",
      "Word: BKS2 -> Predicted Entity: B-LOC\n",
      "Word: BAB -> Predicted Entity: O\n",
      "Word: SV1 -> Predicted Entity: B-LOC\n",
      "Word: SV3 -> Predicted Entity: B-LOC\n",
      "Word: o_SKV1 -> Predicted Entity: O\n",
      "Word: o_SKV6 -> Predicted Entity: O\n",
      "Word: BR -> Predicted Entity: O\n",
      "Word: SKV1 -> Predicted Entity: B-LOC\n",
      "Word: SKV6 -> Predicted Entity: B-LOC\n",
      "Word: SKV3 -> Predicted Entity: B-LOC\n",
      "Word: SKV4 -> Predicted Entity: B-LOC\n",
      "Word: BKS1 -> Predicted Entity: B-LOC\n",
      "Word: BKS6 -> Predicted Entity: B-LOC\n",
      "Word: BFS1 -> Predicted Entity: B-LOC\n",
      "Word: BFS3 -> Predicted Entity: B-LOC\n",
      "Word: o_SKV2 -> Predicted Entity: O\n",
      "Word: BB1 -> Predicted Entity: B-LOC\n",
      "Word: o_SGS1 -> Predicted Entity: O\n",
      "Word: o_SGS2 -> Predicted Entity: O\n",
      "Word: BR -> Predicted Entity: B-LOC\n",
      "Word: SPA -> Predicted Entity: B-LOC\n",
      "Word: GS -> Predicted Entity: B-LOC\n",
      "Word: SGS1 -> Predicted Entity: B-LOC\n",
      "Word: SGS2 -> Predicted Entity: B-LOC\n",
      "Word: SGS1 -> Predicted Entity: B-LOC\n",
      "Word: BR -> Predicted Entity: B-LOC\n",
      "Word: o_BVF -> Predicted Entity: O\n",
      "Word: BIA -> Predicted Entity: B-ORG\n",
      "Word: f_BLK -> Predicted Entity: O\n",
      "Word: BKS1-6 -> Predicted Entity: B-ORG\n",
      "Word: BB1 -> Predicted Entity: B-LOC\n",
      "Word: SVT -> Predicted Entity: O\n",
      "Word: BE -> Predicted Entity: O\n",
      "Word: o_SF1 -> Predicted Entity: O\n",
      "Word: o_SF2 -> Predicted Entity: O\n",
      "Word: o_SV1 -> Predicted Entity: O\n",
      "Word: GF1 -> Predicted Entity: B-LOC\n",
      "Word: GF3 -> Predicted Entity: B-LOC\n",
      "Word: o_SV1 -> Predicted Entity: O\n",
      "Word: GF1 -> Predicted Entity: B-LOC\n",
      "Word: GF3 -> Predicted Entity: B-LOC\n",
      "Word: o_SV1 -> Predicted Entity: O\n",
      "Word: SPA -> Predicted Entity: B-LOC\n",
      "Word: GF -> Predicted Entity: B-LOC\n",
      "Word: SGS -> Predicted Entity: B-LOC\n",
      "Word: GB1 -> Predicted Entity: O\n",
      "Word: SPA -> Predicted Entity: B-LOC\n",
      "Word: BR -> Predicted Entity: B-LOC\n",
      "Word: GB1 -> Predicted Entity: B-LOC\n",
      "Word: o_SV3 -> Predicted Entity: O\n",
      "Word: GB1 -> Predicted Entity: B-LOC\n",
      "Word: o_SV3 -> Predicted Entity: O\n",
      "Word: BKS -> Predicted Entity: B-LOC\n",
      "Word: BR -> Predicted Entity: B-LOC\n",
      "Word: o_SPA -> Predicted Entity: B-LOC\n",
      "Word: GB2 -> Predicted Entity: O\n",
      "Word: o_SKV1 -> Predicted Entity: O\n",
      "Word: o_SV3 -> Predicted Entity: B-LOC\n",
      "Word: H140_1 -> Predicted Entity: B-LOC\n",
      "Word: o_SV1 -> Predicted Entity: O\n",
      "Word: o_SF1-2 -> Predicted Entity: O\n",
      "Word: f_SV2 -> Predicted Entity: O\n",
      "Word: BFS1-2 -> Predicted Entity: B-LOC\n",
      "Word: f_SV3 -> Predicted Entity: O\n",
      "Word: o_SPA -> Predicted Entity: O\n",
      "Word: SV1 -> Predicted Entity: B-LOC\n",
      "Word: SV2 -> Predicted Entity: B-LOC\n",
      "Word: o_SPA -> Predicted Entity: O\n",
      "Word: H320 -> Predicted Entity: O\n",
      "Word: o_SPA -> Predicted Entity: O\n",
      "Word: SV1 -> Predicted Entity: B-LOC\n",
      "Word: SKV1 -> Predicted Entity: B-LOC\n",
      "Word: RV9 -> Predicted Entity: B-LOC\n",
      "Word: SPA -> Predicted Entity: B-LOC\n",
      "Word: SGS -> Predicted Entity: B-LOC\n",
      "Word: SGT -> Predicted Entity: O\n",
      "Word: BLK -> Predicted Entity: B-LOC\n",
      "Word: o_GF1-3 -> Predicted Entity: O\n",
      "Word: RV9 -> Predicted Entity: B-LOC\n",
      "Word: GF1-GF3 -> Predicted Entity: B-LOC\n",
      "Word: SVG -> Predicted Entity: B-ORG\n",
      "Word: GF1 -> Predicted Entity: B-LOC\n",
      "Word: BLK1 -> Predicted Entity: B-LOC\n",
      "Word: BKS1 -> Predicted Entity: B-LOC\n",
      "Word: SPP -> Predicted Entity: O\n",
      "Word: BLK2 -> Predicted Entity: B-LOC\n",
      "Word: B1 -> Predicted Entity: B-LOC\n",
      "Word: B4 -> Predicted Entity: O\n",
      "Word: BKS6 -> Predicted Entity: B-LOC\n",
      "Word: BR -> Predicted Entity: O\n",
      "Word: o_GF3 -> Predicted Entity: O\n",
      "Word: BKS1 -> Predicted Entity: B-PROD\n",
      "Word: #02 -> Predicted Entity: O\n",
      "Word: Fv -> Predicted Entity: B-LOC\n",
      "Word: B1 -> Predicted Entity: O\n",
      "Word: B4 -> Predicted Entity: O\n",
      "Word: GN -> Predicted Entity: O\n",
      "Word: GTD1-2 -> Predicted Entity: B-PROD\n",
      "Word: GB -> Predicted Entity: O\n",
      "Word: o_GB -> Predicted Entity: O\n",
      "Word: GP -> Predicted Entity: B-LOC\n",
      "Word: GV -> Predicted Entity: O\n",
      "Word: f_GV1 -> Predicted Entity: O\n",
      "Word: BKS4 -> Predicted Entity: B-LOC\n",
      "Word: BKS6 -> Predicted Entity: B-LOC\n",
      "Word: VFV -> Predicted Entity: O\n",
      "Word: VB -> Predicted Entity: O\n",
      "Word: H320 -> Predicted Entity: O\n",
      "Word: H320_1 -> Predicted Entity: O\n",
      "Word: BB1 -> Predicted Entity: O\n",
      "Word: H320_2 -> Predicted Entity: O\n",
      "Word: H370 -> Predicted Entity: O\n",
      "Word: H210 -> Predicted Entity: O\n",
      "Word: H220 -> Predicted Entity: O\n",
      "Word: GN -> Predicted Entity: O\n",
      "Word: f_SV2 -> Predicted Entity: O\n",
      "Word: o_SV1 -> Predicted Entity: O\n",
      "Word: f_BLK -> Predicted Entity: O\n",
      "Word: f_SGT -> Predicted Entity: O\n",
      "Word: o_SGS -> Predicted Entity: O\n",
      "Word: RV9 -> Predicted Entity: B-LOC\n",
      "Word: o_GT4 -> Predicted Entity: O\n",
      "Word: o_GT4 -> Predicted Entity: O\n",
      "Word: o_GT2 -> Predicted Entity: O\n",
      "Word: o_GT3 -> Predicted Entity: O\n",
      "Word: o_GT2 -> Predicted Entity: O\n",
      "Word: o_GT6 -> Predicted Entity: O\n",
      "Word: BKS4 -> Predicted Entity: B-LOC\n",
      "Word: BKS5 -> Predicted Entity: B-LOC\n",
      "Word: BKS6 -> Predicted Entity: B-LOC\n",
      "Word: o_GT5 -> Predicted Entity: B-LOC\n",
      "Word: BFS2 -> Predicted Entity: B-LOC\n",
      "Word: f_SV2 -> Predicted Entity: O\n",
      "Word: f_SV3 -> Predicted Entity: O\n",
      "Word: o_SV1 -> Predicted Entity: O\n",
      "Word: BFS2 -> Predicted Entity: B-LOC\n",
      "Word: o_BVF -> Predicted Entity: O\n"
     ]
    }
   ],
   "source": [
    "print(\"Model predictions for B-FELT words:\")\n",
    "for word, pred in zip(b_felt_words, b_felt_preds):\n",
    "    print(f\"Word: {word} -> Predicted Entity: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       1.00      1.00      1.00       121\n",
      "       B-ORG       1.00      1.00      1.00         9\n",
      "      B-PROD       1.00      1.00      1.00        14\n",
      "           O       1.00      1.00      1.00       101\n",
      "\n",
      "    accuracy                           1.00       245\n",
      "   macro avg       1.00      1.00      1.00       245\n",
      "weighted avg       1.00      1.00      1.00       245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(b_felt_preds, b_felt_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
